{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85e8b4cf-b3bd-450e-9785-12fa95e308c4",
   "metadata": {},
   "source": [
    "## [`Retrieval-Augmented Generation` (RAG) using `LangChain`, `LlamaIndex` and `OpenAI`](https://medium.com/@prasadmahamulkar/introduction-to-retrieval-augmented-generation-rag-using-langchain-and-lamaindex-bd0047628e2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08c5a37-a5ab-4270-a13b-6c9111077709",
   "metadata": {},
   "source": [
    "#### How does RAG work?\n",
    "\n",
    "- **Indexing**\n",
    "The indexing process is a crucial first step in data preparation for language models. Original data is cleaned, converted into standardized plain text, and segmented into smaller chunks for efficient processing. These chunks are transformed into vector representations through an embedding model, facilitating similarity comparisons during retrieval. The final index stores these text chunks and their vector embeddings, enabling efficient and scalable search capabilities.\n",
    "\n",
    "- **Retrieval**\n",
    "When a user asks a question, the system uses the encoding model from the indexing phase to transcode it. Next, it calculates similarity scores between the query vector and vectorized chunks within the indexed corpus. The system prioritizes and retrieves the top K chunks showing the highest similarity, using them as an expanded contextual basis to address the user’s request.\n",
    "\n",
    "- **Generation**\n",
    "The user’s question and chosen documents are combined into a clear prompt for a large language model. Then model crafts a response, adapting its approach based on task-specific criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bba8f8-1971-4ab7-9a39-1fd9a883b384",
   "metadata": {},
   "source": [
    "#### Basic RAG Using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a134ec8-a162-4ce4-b635-79a67e6aa101",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers pypdf faiss-gpu\n",
    "!pip install langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1938b9-642e-4aab-820e-623f9ebd2a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start by installing and loading all the necessary libraries.\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# For openai key\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Your Key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e514b1e-3b6c-499d-875e-3528db7aa910",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load a PDF document using PyPDFLoader to extract text from PDF files.\n",
    "\n",
    "loader = PyPDFLoader(\"/content/qlora_paper.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab1f76-67e0-4054-84e2-ff462102196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the TextSplitter to split the document into chunks.\n",
    "\n",
    "text = RecursiveCharacterTextSplitter().split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ebba0-9212-4fc7-a6c6-332d0496e043",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load an embedding model to convert text into numerical embeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\", \n",
    "encode_kwargs={\"normalize_embeddings\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71be375-c393-4483-ac4b-d8964a785e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a Vector Store using FAISS to store embeddings and text chunks.\n",
    "\n",
    "vectorstore = FAISS.from_documents(text, embeddings)\n",
    "\n",
    "## Save these embeddings for later use.\n",
    "\n",
    "vectorstore.save_local(\"vectorstore.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354c448d-572b-40a7-89b8-675931c3f72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a retriever using the vector store. Establishes the foundation for information retrieval based on vector similarities.\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77752a0d-ab3b-4787-bca4-465d99158c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the Language Model (LLM) to use for retrieval and create a document chain.\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "template = \"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the provided context only to answer the following question:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "doc_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, doc_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f1e4d-17ab-44bf-99a6-17382e53ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a retrieval chain by combining the retriever and document chain.\n",
    "## Invoke the chain with a user query to get a relevant response.\n",
    "\n",
    "response = chain.invoke({\"input\": \"what is Qlora?\"})\n",
    "\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8b7c79-aa50-46be-b6e8-65a8ae9c9c7e",
   "metadata": {},
   "source": [
    "#### Basic RAG with `LlamaIndex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811999fa-1167-4ca3-950b-7379b517c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install and load all the necessary libraries from llamaIndex.\n",
    "\n",
    "!pip install -U llama_hub llama_index pypdf\n",
    "\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index import Document\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.schema import IndexNode\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import ServiceContext\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# For openai key\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Your Key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc901b6-2256-4e1c-8c36-e19caa315cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load a PDF document and combine each page of the document into one document object.\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "input_files=[\"/content/qlora_paper.pdf\"]).load_data()\n",
    "\n",
    "doc_text = \"\\n\\n\".join([d.get_content() for d in documents])\n",
    "text= [Document(text=doc_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a395bb-dadd-403b-94e9-74644cbcead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the document into text chunks. Reset default node IDs for better understanding.\n",
    "\n",
    "node_parser = SimpleNodeParser.from_defaults()\n",
    "base_nodes = node_parser.get_nodes_from_documents(text)\n",
    "\n",
    "for idx, node in enumerate(base_nodes):\n",
    "    node.id_ = f\"node-{idx}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a26b39b-16a4-40cb-8609-16ca5a85b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load an embedding model and language model (LLM)\n",
    "\n",
    "embed_model = resolve_embed_model(\"local:BAAI/bge-small-en\")\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aadba8-63ec-48b7-96fe-18b1b80ccdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a service by bundling LLM and embedding model for the indexing and querying stage.\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdfb761-d597-4957-a25f-b6f71ae14c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create and store embeddings of nodes (chunks) and store them in the vector store index.\n",
    "\n",
    "index = VectorStoreIndex(base_nodes, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10de7641-6b64-46a5-a078-b1e484117ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a retriever using the vector store index to retrieve relevant information for user queries.\n",
    "\n",
    "retriever = index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a268938c-88f7-4dca-a903-34ccd031e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up a query engine by combining the retriever and service context, and add a user query to get a relevant response.\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, service_context=service_context)\n",
    "\n",
    "response = query_engine.query(\"What is Qlora?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f0bd4c-3ef6-400b-9d53-2c3462b06abe",
   "metadata": {},
   "source": [
    "#### Advanced RAG Using `LangChain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b189e2-9703-411b-bba8-bf6d5f41e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the TextSplitter to split the document into parent and child chunks.\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "## Create the parent documents - The big chunks\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "\n",
    "## Create the child documents - The small chunks\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "from langchain.storage import InMemoryStore\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3bbb8a-bf0b-46dc-b4b2-64cddaa16e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a Vector Store using Chromadb to store new embeddings and text chunks.\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma(collection_name=\"split_parents\", \n",
    "embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c05ae-8982-42ed-8ff5-2f7201987ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a Parent doc retriever then, add a document to the retriever.\n",
    "\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "\n",
    "retriever.add_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58273e4-4baa-40d3-9ef3-62c5a6599fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a retrieval chain, similar to the previous chain, and invoke it with a user query to get a response.\n",
    "\n",
    "response = chain.invoke({\"input\": \"what is Qlora?\"})\n",
    "\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cba405a-e764-4262-a9c6-b7d293bb07f0",
   "metadata": {},
   "source": [
    "#### Advanced RAG Using `LlamaIndex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6119699c-7bd1-4988-ab0d-099656e8b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set child chunk sizes (128, 256, 512) in `sub_chunk_sizes` and create parsers (`sub_node_parsers`) for child chunks\n",
    "\n",
    "sub_chunk_sizes = [128, 256, 512]\n",
    "sub_node_parsers = [\n",
    "    SimpleNodeParser.from_defaults(chunk_size=c,chunk_overlap=20) for c in sub_chunk_sizes\n",
    "]\n",
    "\n",
    "all_nodes = []\n",
    "for base_node in base_nodes:\n",
    "    for n in sub_node_parsers:\n",
    "        sub_nodes = n.get_nodes_from_documents([base_node])\n",
    "        sub_inodes = [\n",
    "            IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes\n",
    "        ]\n",
    "        all_nodes.extend(sub_inodes)\n",
    "\n",
    "    # also add original node to node\n",
    "    original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n",
    "    all_nodes.append(original_node)\n",
    "\n",
    "all_nodes_dict = {n.node_id: n for n in all_nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33a2ca-8036-44c9-8a73-e8767b2268d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create embeddings of all nodes (which contain both parent and child nodes) and store them in the vector store index.\n",
    "\n",
    "index = VectorStoreIndex(all_nodes, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa79086a-49f5-4a07-ae54-621b2dd7b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "\n",
    "vector_retriever_chunk = index.as_retriever()\n",
    "\n",
    "from llama_index.retrievers import RecursiveRetriever\n",
    "retriever_chunk = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": vector_retriever_chunk},\n",
    "    node_dict=all_nodes_dict,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c955bac5-2f0a-4ed9-9451-4db778bdaa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up a query engine by combining the retriever and service context, and add a user query.\n",
    "\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine_chunk = RetrieverQueryEngine.from_args(retriever_chunk, \n",
    "service_context=service_context)\n",
    "\n",
    "response = query_engine_chunk.query(\"What is Qlora?\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
